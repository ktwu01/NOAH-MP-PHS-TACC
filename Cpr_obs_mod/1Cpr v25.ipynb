{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean the requirements file by removing file-based dependencies \n",
    "# !grep -v \"@ file:\" requirements.txt > clean_requirements.txt\n",
    "\n",
    "# # Install packages from the cleaned requirements file\n",
    "# !pip install -r clean_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: numpy<1.25.0 in /home/jovyan/.local/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already up-to-date: scipy in /home/jovyan/.local/lib/python3.8/site-packages (1.10.1)\n",
      "Requirement already up-to-date: numba in /home/jovyan/.local/lib/python3.8/site-packages (0.58.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.9\" in /opt/conda/lib/python3.8/site-packages (from numba) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: llvmlite<0.42,>=0.41.0dev0 in /home/jovyan/.local/lib/python3.8/site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata; python_version < \"3.9\"->numba) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/lib/python3.8/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.9\"->numba) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"numpy<1.25.0\" scipy numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting importlib_metadata==4.13.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib_metadata==4.13.0) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.8/site-packages (from zipp>=0.5->importlib_metadata==4.13.0) (7.2.0)\n",
      "Installing collected packages: importlib-metadata\n",
      "Successfully installed importlib-metadata-4.13.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: netcdf4 in /home/jovyan/.local/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already satisfied: cftime in /home/jovyan/.local/lib/python3.8/site-packages (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from netcdf4) (2020.11.8)\n",
      "Requirement already satisfied: numpy in /home/jovyan/.local/lib/python3.8/site-packages (from netcdf4) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "# Fix importlib_metadata compatibility issue\n",
    "!pip install importlib_metadata==4.13.0\n",
    "\n",
    "# Install xarray dependencies\n",
    "!pip install netcdf4 cftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xarray==2023.1.0\n",
      "  Using cached xarray-2023.1.0-py3-none-any.whl (973 kB)\n",
      "Collecting packaging>=21.3\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting pandas>=1.3\n",
      "  Using cached pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[K     |████████████████████████████████| 229 kB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: packaging, numpy, tzdata, six, python-dateutil, pytz, pandas, xarray\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.2\n",
      "    Uninstalling packaging-24.2:\n",
      "      Successfully uninstalled packaging-24.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "  Attempting uninstall: xarray\n",
      "    Found existing installation: xarray 2023.1.0\n",
      "    Uninstalling xarray-2023.1.0:\n",
      "      Successfully uninstalled xarray-2023.1.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.24.4 which is incompatible.\n",
      "tapipy 0.3.0 requires setuptools<22.0.0,>=21.0.0, but you'll have setuptools 75.3.2 which is incompatible.\n",
      "nbclient 0.5.1 requires jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.4 which is incompatible.\n",
      "nbclient 0.5.1 requires nbformat>=5.0, but you'll have nbformat 4.4.0 which is incompatible.\n",
      "jupyterhub 1.2.1 requires jupyter-telemetry>=0.1.0, but you'll have jupyter-telemetry 0.0.5 which is incompatible.\u001b[0m\n",
      "Successfully installed numpy-1.24.4 packaging-24.2 pandas-2.0.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2 xarray-2023.1.0\n"
     ]
    }
   ],
   "source": [
    "# Reinstall xarray with compatible dependencies\n",
    "!pip install xarray==2023.1.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install netCDF4 xarray matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: numexpr in /home/jovyan/.local/lib/python3.8/site-packages (2.8.6)\n",
      "Requirement already up-to-date: bottleneck in /home/jovyan/.local/lib/python3.8/site-packages (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /home/jovyan/.local/lib/python3.8/site-packages (from numexpr) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numexpr bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_latent_heat_data(obs_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for latent heat.\"\"\"\n",
    "    ds = xr.open_dataset(obs_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values], \n",
    "                             format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return ds['Qle_cor'].to_dataframe()\n",
    "\n",
    "def process_sensible_heat_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for sensible heat.\"\"\"\n",
    "    ds_flux = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds_flux.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds_flux.time.values], \n",
    "                             format='%Y%m%d%H')\n",
    "        ds_flux['time'] = times\n",
    "    return pd.DataFrame({'Qh': ds_flux['Qh_cor'].values}, index=ds_flux.time.values)\n",
    "\n",
    "def process_gpp_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for GPP.\"\"\"\n",
    "    ds_flux = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds_flux.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds_flux.time.values], \n",
    "                             format='%Y%m%d%H')\n",
    "        ds_flux['time'] = times\n",
    "    return pd.DataFrame({'GPP': ds_flux['GPP_DT'].values}, index=ds_flux.time.values)\n",
    "\n",
    "def process_model_data(mod_file: str) -> tuple:\n",
    "    \"\"\"Process model data for all variables.\"\"\"\n",
    "    ds = xr.open_dataset(mod_file)\n",
    "    times = [datetime.strptime(t.decode('utf-8'), '%Y-%m-%d_%H:%M:%S') \n",
    "             for t in ds['Times'].values]\n",
    "    return (\n",
    "        pd.DataFrame({'LH': ds['LH'][:, 0, 0].values}, index=times),\n",
    "        pd.DataFrame({'HFX': ds['HFX'][:, 0, 0].values}, index=times),\n",
    "        pd.DataFrame({'PSN': ds['PSN'][:, 0, 0].values}, index=times)\n",
    "    )\n",
    "\n",
    "def split_data_by_index(data: pd.DataFrame, split_index: int) -> tuple:\n",
    "    \"\"\"Split data into two periods based on index.\"\"\"\n",
    "    data1 = data.iloc[:split_index]\n",
    "    data2 = data.iloc[split_index:]\n",
    "    return data1, data2\n",
    "\n",
    "def analyze_data_statistics(data: pd.DataFrame, column: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyze and print statistics for the original data.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input dataframe\n",
    "        column (str): Column name to analyze\n",
    "        name (str): Name of the dataset for printing\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name} Statistics:\")\n",
    "    print(f\"Max value: {data[column].max():.2f}\")\n",
    "    print(f\"Min value: {data[column].min():.2f}\")\n",
    "    print(f\"Mean value: {data[column].mean():.2f}\")\n",
    "    print(f\"Time of max value: {data[column].idxmax()}\")\n",
    "    print(f\"Time of min value: {data[column].idxmin()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_seasonal_data(df: pd.DataFrame, column: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Create seasonal data by aggregating values for each day of year.\n",
    "    Returns both the seasonal data and its statistics.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe with datetime index\n",
    "        column (str): Column name to process\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (seasonal_data, daily_stats)\n",
    "    \"\"\"\n",
    "    df_seasonal = df.copy()\n",
    "    df_seasonal['year'] = df_seasonal.index.year\n",
    "    df_seasonal['doy'] = df_seasonal.index.dayofyear\n",
    "    \n",
    "    # Group by day of year and calculate statistics\n",
    "    daily_stats = df_seasonal.groupby('doy')[column].agg(['mean', 'min', 'max'])\n",
    "    \n",
    "    # Calculate individual year values for plotting\n",
    "    seasonal_years = df_seasonal.groupby(['year', 'doy'])[column].mean().unstack(0)\n",
    "    \n",
    "    return seasonal_years, daily_stats\n",
    "\n",
    "def create_diurnal_data(df: pd.DataFrame, column: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Create diurnal data by aggregating values for each hour of day in local time.\n",
    "    Returns both the diurnal data (individual years) and hourly statistics.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with datetime index in UTC.\n",
    "        column (str): Column name to process.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (diurnal_data, hourly_stats)\n",
    "    \"\"\"\n",
    "#     ！！！注意！如果需要其他时区的站点，需要修改！！！\n",
    "#     这里假设记录的时间是 UTC，而我们希望将其转换为 Site 的 local time。\n",
    "#   local longitude = 90W\n",
    "#   local_offset = -90 / 15 = -6 hours.\n",
    "\n",
    "    local_offset = -6  # Offset in hours from UTC to local time\n",
    "    df_diurnal = df.copy()\n",
    "    df_diurnal['year'] = df_diurnal.index.year\n",
    "    # Adjust the hour to local time by adding the offset and wrapping with modulo 24\n",
    "    df_diurnal['hour'] = (df_diurnal.index.hour + local_offset) % 24\n",
    "\n",
    "    # Group by local hour and calculate statistics\n",
    "    hourly_stats = df_diurnal.groupby('hour')[column].agg(['mean', 'min', 'max'])\n",
    "    \n",
    "    # Calculate individual year values for plotting (grouped by year and local hour)\n",
    "    diurnal_years = df_diurnal.groupby(['year', 'hour'])[column].mean().unstack(0)\n",
    "    \n",
    "    return diurnal_years, hourly_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_old(ax, seasonal_data, daily_stats, is_obs=True):\n",
    "    \"\"\"\n",
    "    绘制季节性（日内）数据，不自动添加 legend。\n",
    "    \"\"\"\n",
    "    # 统一颜色：观测用 #1f77b4，模式用 #ff7f0e\n",
    "    color = '#1f77b4' if is_obs else '#ff7f0e'\n",
    "    \n",
    "    # 绘制每年的散点（代表每日均值）\n",
    "    for _, year in enumerate(seasonal_data.columns):\n",
    "        ax.scatter(seasonal_data.index, seasonal_data[year],\n",
    "                   color=color, alpha=0.3, s=1)\n",
    "    \n",
    "    # 绘制所有年份的均值折线，\n",
    "    ax.plot(daily_stats.index, daily_stats['mean'], color=color,\n",
    "            linewidth=2, alpha=0.79)\n",
    "    # # 并填充 min/max 区域\n",
    "    # ax.fill_between(daily_stats.index, daily_stats['min'], daily_stats['max'],\n",
    "    #                 color=color, alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Day of Year')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal(ax, seasonal_data, daily_stats, is_obs=True):\n",
    "    \"\"\"\n",
    "    绘制季节性（日内）数据，不自动添加 legend。\n",
    "    \"\"\"\n",
    "    # 统一颜色：观测用 #1f77b4，模式用 #ff7f0e\n",
    "    color = '#1f77b4' if is_obs else '#ff7f0e'\n",
    "    \n",
    "    # 绘制每年的散点（代表每日均值）\n",
    "    for _, year in enumerate(seasonal_data.columns):\n",
    "        # Convert index to numpy array\n",
    "        x_values = seasonal_data.index.to_numpy()\n",
    "        y_values = seasonal_data[year].values\n",
    "        ax.scatter(x_values, y_values, color=color, alpha=0.3, s=1)\n",
    "    \n",
    "    # 绘制所有年份的均值折线，\n",
    "    # Convert index to numpy array\n",
    "    x_values = daily_stats.index.to_numpy()\n",
    "    y_values = daily_stats['mean'].values\n",
    "    ax.plot(x_values, y_values, color=color, linewidth=2, alpha=0.79)\n",
    "    # # 并填充 min/max 区域\n",
    "    # ax.fill_between(daily_stats.index, daily_stats['min'], daily_stats['max'],\n",
    "    #                 color=color, alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Day of Year')\n",
    "    ax.grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diurnal_old(ax, diurnal_obs, hourly_stats_obs, diurnal_mod, hourly_stats_mod, show_legend=False):\n",
    "    \"\"\"\n",
    "    Plot diurnal variation on the given (twin) axis.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axis (typically a twin x-axis) to plot diurnal data.\n",
    "        diurnal_obs: DataFrame with diurnal data (observations) with index=hour.\n",
    "        hourly_stats_obs: DataFrame with hourly statistics for observations.\n",
    "        diurnal_mod: DataFrame with diurnal data (model) with index=hour.\n",
    "        hourly_stats_mod: DataFrame with hourly statistics for model.\n",
    "        show_legend: Boolean; if True, add the legend.\n",
    "    \"\"\"\n",
    "    # Choose diurnal colors different from seasonal colors\n",
    "    obs_color = '#7f7f7f'  # e.g., grey for observations\n",
    "    mod_color = '#e377c2'  # e.g., magenta/pink for model data\n",
    "    # obs_color = '#7f7f7f'  # e.g., grey for observations\n",
    "    # mod_color = '#bcbd22'  # e.g., olive for model data\n",
    "\n",
    "    # --- Plot observed diurnal data ---\n",
    "    # Scatter points for each year; only the last (iterated) column gets a label.\n",
    "    for i, year in enumerate(diurnal_obs.columns):\n",
    "        label = 'Obs Diurnal' if i == len(diurnal_obs.columns) - 1 else None\n",
    "        ax.scatter(diurnal_obs.index, diurnal_obs[year], color=obs_color, alpha=0.3,\n",
    "                   s=5, label=label)\n",
    "    \n",
    "    # Plot the diurnal mean as a line and fill the min/max envelope.\n",
    "    # (No label is added so that only the scatter marker shows in the legend.)\n",
    "    ax.plot(hourly_stats_obs.index, hourly_stats_obs['mean'], color=obs_color,\n",
    "            linewidth=2, label=None)\n",
    "    # ax.fill_between(hourly_stats_obs.index, hourly_stats_obs['min'], hourly_stats_obs['max'],\n",
    "    #                 color=obs_color, alpha=0.1)\n",
    "\n",
    "    # --- Plot modeled diurnal data ---\n",
    "    for i, year in enumerate(diurnal_mod.columns):\n",
    "        label = 'Mod Diurnal' if i == len(diurnal_mod.columns) - 1 else None\n",
    "        ax.scatter(diurnal_mod.index, diurnal_mod[year], color=mod_color, alpha=0.3,\n",
    "                   s=5, label=label)\n",
    "    \n",
    "    ax.plot(hourly_stats_mod.index, hourly_stats_mod['mean'], color=mod_color,\n",
    "            linewidth=2, label=None)\n",
    "    # ax.fill_between(hourly_stats_mod.index, hourly_stats_mod['min'], hourly_stats_mod['max'],\n",
    "    #                 color=mod_color, alpha=0.1)\n",
    "\n",
    "    ax.set_xlabel('Local Hour of Day', labelpad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if show_legend:\n",
    "        # With the labels set only on the scatter markers, the legend will show four items:\n",
    "        # seasonal obs (from plot_seasonal), seasonal mod (from plot_seasonal),\n",
    "        # diurnal obs, and diurnal mod.\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0, 1), ncol=2, framealpha=1, markerscale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diurnal(ax, diurnal_obs, hourly_stats_obs, diurnal_mod, hourly_stats_mod, show_legend=False):\n",
    "    \"\"\"\n",
    "    Plot diurnal variation on the given (twin) axis.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axis (typically a twin x-axis) to plot diurnal data.\n",
    "        diurnal_obs: DataFrame with diurnal data (observations) with index=hour.\n",
    "        hourly_stats_obs: DataFrame with hourly statistics for observations.\n",
    "        diurnal_mod: DataFrame with diurnal data (model) with index=hour.\n",
    "        hourly_stats_mod: DataFrame with hourly statistics for model.\n",
    "        show_legend: Boolean; if True, add the legend.\n",
    "    \"\"\"\n",
    "    # Choose diurnal colors different from seasonal colors\n",
    "    obs_color = '#7f7f7f'  # e.g., grey for observations\n",
    "    mod_color = '#e377c2'  # e.g., magenta/pink for model data\n",
    "    \n",
    "    # --- Plot observed diurnal data ---\n",
    "    # Scatter points for each year; only the last (iterated) column gets a label.\n",
    "    for i, year in enumerate(diurnal_obs.columns):\n",
    "        label = 'Obs Diurnal' if i == len(diurnal_obs.columns) - 1 else None\n",
    "        # Convert index to numpy array\n",
    "        x_values = diurnal_obs.index.to_numpy()\n",
    "        y_values = diurnal_obs[year].values\n",
    "        ax.scatter(x_values, y_values, color=obs_color, alpha=0.3, s=5, label=label)\n",
    "    \n",
    "    # Plot the diurnal mean as a line\n",
    "    x_values = hourly_stats_obs.index.to_numpy()\n",
    "    y_values = hourly_stats_obs['mean'].values\n",
    "    ax.plot(x_values, y_values, color=obs_color, linewidth=2, label=None)\n",
    "    \n",
    "    # --- Plot modeled diurnal data ---\n",
    "    for i, year in enumerate(diurnal_mod.columns):\n",
    "        label = 'Mod Diurnal' if i == len(diurnal_mod.columns) - 1 else None\n",
    "        # Convert index to numpy array\n",
    "        x_values = diurnal_mod.index.to_numpy()\n",
    "        y_values = diurnal_mod[year].values\n",
    "        ax.scatter(x_values, y_values, color=mod_color, alpha=0.3, s=5, label=label)\n",
    "    \n",
    "    # Plot the model mean line\n",
    "    x_values = hourly_stats_mod.index.to_numpy()\n",
    "    y_values = hourly_stats_mod['mean'].values\n",
    "    ax.plot(x_values, y_values, color=mod_color, linewidth=2, label=None)\n",
    "    \n",
    "    ax.set_xlabel('Local Hour of Day', labelpad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if show_legend:\n",
    "        ax.legend(loc='upper left', bbox_to_anchor=(0, 1), ncol=2, framealpha=1, markerscale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_old(ax, obs1, obs2, mod1, mod2, obs_col, mod_col, ylabel, show_legend=False):\n",
    "    ax.plot(obs1.index, obs1[obs_col], '.', color='#1f77b4',\n",
    "            label='Obs (’02-’07)', alpha=0.79, markersize=2)\n",
    "    ax.plot(mod1.index, mod1[mod_col], '.', color='#ff7f0e',\n",
    "            label='Mod (’02-’07)', alpha=0.69, markersize=2)\n",
    "    ax.plot(obs2.index, obs2[obs_col], '.', color='#1f77b4',\n",
    "            label='Obs (’08-’09)', alpha=0.2, markersize=2)\n",
    "    ax.plot(mod2.index, mod2[mod_col], '.', color='#ff7f0e',\n",
    "            label='Mod (’08-’09)', alpha=0.2, markersize=2)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if show_legend:\n",
    "        ax.legend(loc='upper right', ncol=4, framealpha=0.5, markerscale=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(ax, obs1, obs2, mod1, mod2, obs_col, mod_col, ylabel, show_legend=False):\n",
    "    # Convert pandas DatetimeIndex to numpy array before plotting\n",
    "    # This avoids the multi-dimensional indexing error\n",
    "    obs1_times = obs1.index.to_numpy()\n",
    "    obs2_times = obs2.index.to_numpy()\n",
    "    mod1_times = mod1.index.to_numpy()\n",
    "    mod2_times = mod2.index.to_numpy()\n",
    "    \n",
    "    ax.plot(obs1_times, obs1[obs_col].values, '.', color='#1f77b4',\n",
    "            label=\"Obs ('02-'07)\", alpha=0.79, markersize=2)\n",
    "    ax.plot(mod1_times, mod1[mod_col].values, '.', color='#ff7f0e',\n",
    "            label=\"Mod ('02-'07)\", alpha=0.69, markersize=2)\n",
    "    ax.plot(obs2_times, obs2[obs_col].values, '.', color='#1f77b4',\n",
    "            label=\"Obs ('08-'09)\", alpha=0.2, markersize=2)\n",
    "    ax.plot(mod2_times, mod2[mod_col].values, '.', color='#ff7f0e',\n",
    "            label=\"Mod ('08-'09)\", alpha=0.2, markersize=2)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if show_legend:\n",
    "        ax.legend(loc='upper right', ncol=4, framealpha=0.5, markerscale=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(ax, obs1, mod1, obs_col, mod_col, show_legend=False):\n",
    "    common_times = obs1.index.intersection(mod1.index)\n",
    "    correlation = stats.pearsonr(\n",
    "        obs1.loc[common_times, obs_col].values,\n",
    "        mod1.loc[common_times, mod_col].values\n",
    "    )[0]\n",
    "    \n",
    "    ax.scatter(obs1.loc[common_times, obs_col],\n",
    "               mod1.loc[common_times, mod_col],\n",
    "               alpha=0.5, color='#1f77b4', s=10)\n",
    "    \n",
    "    lims = [\n",
    "        np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "        np.max([ax.get_xlim(), ax.get_ylim()])\n",
    "    ]\n",
    "    ax.plot(lims, lims, 'k-', alpha=1, zorder=0, label='1:1 line')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if show_legend:\n",
    "        ax.legend()\n",
    "    return correlation\n",
    "\n",
    "def create_combined_visualization(df_obs_lh: pd.DataFrame, df_mod_lh: pd.DataFrame,\n",
    "                                  df_obs_sh: pd.DataFrame, df_mod_sh: pd.DataFrame,\n",
    "                                  df_obs_gpp: pd.DataFrame, df_mod_psn: pd.DataFrame,\n",
    "                                  split_index: int,\n",
    "                                  figure_dpi: int = 60,\n",
    "                                  savefig_dpi: int = 60) -> None:\n",
    "    \"\"\"创建包含所有分析结果的 3x3 图形\"\"\"\n",
    "    # 设置图像参数\n",
    "    plt.rcParams['figure.dpi'] = figure_dpi\n",
    "    plt.rcParams['savefig.dpi'] = savefig_dpi\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['axes.titlesize'] = 16\n",
    "    plt.rcParams['axes.labelsize'] = 14\n",
    "    plt.rcParams['xtick.labelsize'] = 13\n",
    "    plt.rcParams['ytick.labelsize'] = 13\n",
    "    plt.rcParams['legend.fontsize'] = 13\n",
    "    \n",
    "    # 建立 GridSpec 布局\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = gridspec.GridSpec(3, 3, width_ratios=[3, 2, 1])\n",
    "    \n",
    "    # 数据分割（分为两个时间段）\n",
    "    obs_lh1, obs_lh2 = split_data_by_index(df_obs_lh, split_index)\n",
    "    mod_lh1, mod_lh2 = split_data_by_index(df_mod_lh, split_index)\n",
    "    obs_sh1, obs_sh2 = split_data_by_index(df_obs_sh, split_index)\n",
    "    mod_sh1, mod_sh2 = split_data_by_index(df_mod_sh, split_index)\n",
    "    obs_gpp1, obs_gpp2 = split_data_by_index(df_obs_gpp, split_index)\n",
    "    mod_psn1, mod_psn2 = split_data_by_index(df_mod_psn, split_index)\n",
    "    \n",
    "    # 打印原始数据统计信息\n",
    "    print(\"\\n=== Original Data Statistics ===\")\n",
    "    analyze_data_statistics(obs_lh1, 'Qle_cor', 'Observed Latent Heat')\n",
    "    analyze_data_statistics(mod_lh1, 'LH', 'Modeled Latent Heat')\n",
    "    analyze_data_statistics(obs_sh1, 'Qh', 'Observed Sensible Heat')\n",
    "    analyze_data_statistics(mod_sh1, 'HFX', 'Modeled Sensible Heat')\n",
    "    analyze_data_statistics(obs_gpp1, 'GPP', 'Observed GPP')\n",
    "    analyze_data_statistics(mod_psn1, 'PSN', 'Modeled PSN')\n",
    "    \n",
    "    # 创建季节性数据\n",
    "    seasonal_obs_lh, daily_stats_obs_lh = create_seasonal_data(obs_lh1, 'Qle_cor')\n",
    "    seasonal_mod_lh, daily_stats_mod_lh = create_seasonal_data(mod_lh1, 'LH')\n",
    "    seasonal_obs_sh, daily_stats_obs_sh = create_seasonal_data(obs_sh1, 'Qh')\n",
    "    seasonal_mod_sh, daily_stats_mod_sh = create_seasonal_data(mod_sh1, 'HFX')\n",
    "    seasonal_obs_gpp, daily_stats_obs_gpp = create_seasonal_data(obs_gpp1, 'GPP')\n",
    "    seasonal_mod_gpp, daily_stats_mod_gpp = create_seasonal_data(mod_psn1, 'PSN')\n",
    "    \n",
    "    # 创建日变化数据（仅对第一时间段）\n",
    "    diurnal_obs_lh, hourly_stats_obs_lh = create_diurnal_data(obs_lh1, 'Qle_cor')\n",
    "    diurnal_mod_lh, hourly_stats_mod_lh = create_diurnal_data(mod_lh1, 'LH')\n",
    "    diurnal_obs_sh, hourly_stats_obs_sh = create_diurnal_data(obs_sh1, 'Qh')\n",
    "    diurnal_mod_sh, hourly_stats_mod_sh = create_diurnal_data(mod_sh1, 'HFX')\n",
    "    diurnal_obs_gpp, hourly_stats_obs_gpp = create_diurnal_data(obs_gpp1, 'GPP')\n",
    "    diurnal_mod_gpp, hourly_stats_mod_gpp = create_diurnal_data(mod_psn1, 'PSN')\n",
    "    \n",
    "    # （此处略去其他统计打印代码……）\n",
    "    \n",
    "    # 定义子图\n",
    "    # Row 1: Latent Heat\n",
    "    ax_lh_ts = fig.add_subplot(gs[0, 0])      # 时间序列\n",
    "    ax_lh_seasonal = fig.add_subplot(gs[0, 1])  # 季节性图（主 x 轴为 day-of-year）\n",
    "    ax_lh_scatter = fig.add_subplot(gs[0, 2])   # 散点图\n",
    "    \n",
    "    # Row 2: Sensible Heat\n",
    "    ax_sh_ts = fig.add_subplot(gs[1, 0])\n",
    "    ax_sh_seasonal = fig.add_subplot(gs[1, 1])\n",
    "    ax_sh_scatter = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    # Row 3: GPP\n",
    "    ax_gpp_ts = fig.add_subplot(gs[2, 0])\n",
    "    ax_gpp_seasonal = fig.add_subplot(gs[2, 1])\n",
    "    ax_gpp_scatter = fig.add_subplot(gs[2, 2])\n",
    "    \n",
    "    # 绘制时间序列图（保持原样）\n",
    "    plot_time_series(ax_lh_ts, obs_lh1, obs_lh2, mod_lh1, mod_lh2,\n",
    "                     'Qle_cor', 'LH', 'Latent Heat (W/m²)', True)\n",
    "    plot_time_series(ax_sh_ts, obs_sh1, obs_sh2, mod_sh1, mod_sh2,\n",
    "                     'Qh', 'HFX', 'Sensible Heat (W/m²)', False)\n",
    "    plot_time_series(ax_gpp_ts, obs_gpp1, obs_gpp2, mod_psn1, mod_psn2,\n",
    "                     'GPP', 'PSN', 'GPP (μmol/m²/s)', False)\n",
    "    \n",
    "    # --- 潜热图（第一行） ---\n",
    "    # 在 ax_lh_seasonal 绘制季节性数据（观测和模式）\n",
    "    plot_seasonal(ax_lh_seasonal, seasonal_obs_lh, daily_stats_obs_lh, is_obs=True)\n",
    "    plot_seasonal(ax_lh_seasonal, seasonal_mod_lh, daily_stats_mod_lh, is_obs=False)\n",
    "    \n",
    "    # 创建一个 twinx 轴用于绘制日变化数据（转换为本地时间）\n",
    "    ax_lh_diurnal = ax_lh_seasonal.twiny()\n",
    "    ax_lh_diurnal.set_xlim(0, 23)\n",
    "    plot_diurnal(ax_lh_diurnal, diurnal_obs_lh, hourly_stats_obs_lh, diurnal_mod_lh, hourly_stats_mod_lh)\n",
    "    \n",
    "    # --- 添加自定义 legend ---\n",
    "\n",
    "    # 颜色\n",
    "    obs_season_color = '#1f77b4'\n",
    "    mod_season_color = '#ff7f0e'\n",
    "    obs_diurnal_color = '#7f7f7f'  # e.g., grey for observations\n",
    "    mod_diurnal_color = '#e377c2'  # e.g., magenta/pink for model data\n",
    "\n",
    "    # 定义一个辅助函数，生成由“填充区域+折线+散点”组成的复合图案\n",
    "    def create_composite_handle(color):\n",
    "        # fill = Patch(facecolor=color, edgecolor=color, alpha=0.2)\n",
    "        line = Line2D([0], [0], color=color, linewidth=2)\n",
    "        scatter = Line2D([0], [0], marker='o', color=color, linestyle='None', markersize=5)\n",
    "        # return (fill, line, scatter)\n",
    "        return (line, scatter)\n",
    "\n",
    "#     #定义一组顶点（vertices）和对应的绘制指令（codes），构造出一个类似波浪形状的不规则图形。例如：\n",
    "#     Path = mpath.Path\n",
    "#     vertices = [\n",
    "#         (0.0, 0.0), (0.2, 0.1), (0.4, -0.1), (0.6, 0.1), (0.8, -0.1), (1.0, 0.0),\n",
    "#         (1.0, 1.0), (0.8, 0.9), (0.6, 1.1), (0.4, 0.9), (0.2, 1.1), (0.0, 1.0),\n",
    "#         (0.0, 0.0)  # 闭合形状\n",
    "#     ]\n",
    "#     codes = [\n",
    "#         Path.MOVETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CURVE3,\n",
    "#         Path.LINETO, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CURVE3, Path.CURVE3,\n",
    "#         Path.CLOSEPOLY\n",
    "#     ]\n",
    "    \n",
    "#     wavy_path = mpath.Path(vertices, codes)\n",
    "#     wavy_patch = mpatches.PathPatch(wavy_path, facecolor=obs_season_color, \n",
    "#                                      edgecolor=obs_season_color, alpha=0.2)\n",
    "\n",
    "    # 四个 legend 项：观测季节性、模式季节性、观测日变化、模式日变化\n",
    "    legend_handles = [\n",
    "        create_composite_handle(obs_season_color),\n",
    "        create_composite_handle(mod_season_color),\n",
    "        create_composite_handle(obs_diurnal_color),\n",
    "        create_composite_handle(mod_diurnal_color),\n",
    "        # Patch(facecolor=obs_season_color, edgecolor=obs_season_color, alpha=0.2), # 方块\n",
    "        # wavy_patch,\n",
    "        Line2D([0], [0], marker='o', color=obs_season_color, linestyle='None', markersize=5),\n",
    "        Line2D([0], [0], color=obs_season_color, linewidth=2)\n",
    "    ]\n",
    "    legend_labels = ['Observation Seasonal', 'Modeling Seasonal', 'Observation Diurnal', 'Modeling Diurnal', 'Daily Avg', '6-yr Avg']\n",
    "    \n",
    "    # 将 legend 添加到第一行左上角，这里选择在 ax_lh_seasonal 上添加\n",
    "    ax_lh_seasonal.legend(legend_handles, legend_labels,\n",
    "                          handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "                          loc='upper left', ncol=1)\n",
    "    \n",
    "    # Scatter plot with correlation for Latent Heat\n",
    "    corr_lh = plot_scatter(ax_lh_scatter, obs_lh1, mod_lh1, 'Qle_cor', 'LH', show_legend=True)\n",
    "    ax_lh_seasonal.set_title('(b) Latent Heat Seasonal+Diurnal (’02-’07)')\n",
    "    ax_lh_ts.set_title('(a) Latent Heat Time Series (resolution: hour)')\n",
    "    ax_lh_scatter.set_title(f'(c) Latent Heat Scatter\\nCorrelation (’02-’07): {corr_lh:.3f}')\n",
    "    ax_lh_scatter.set_xlabel('Observed Latent Heat (W/m²)')\n",
    "    ax_lh_scatter.set_ylabel('Modeled Latent Heat (W/m²)')\n",
    "    \n",
    "    # --- Sensible Heat 和 GPP 部分（保持原有绘制，不做 legend 调整） ---\n",
    "    plot_seasonal(ax_sh_seasonal, seasonal_obs_sh, daily_stats_obs_sh, is_obs=True)\n",
    "    plot_seasonal(ax_sh_seasonal, seasonal_mod_sh, daily_stats_mod_sh, is_obs=False)\n",
    "    ax_sh_diurnal = ax_sh_seasonal.twiny()\n",
    "    ax_sh_diurnal.set_xlim(0, 23)\n",
    "    plot_diurnal(ax_sh_diurnal, diurnal_obs_sh, hourly_stats_obs_sh, diurnal_mod_sh, hourly_stats_mod_sh)\n",
    "    \n",
    "    corr_sh = plot_scatter(ax_sh_scatter, obs_sh1, mod_sh1, 'Qh', 'HFX', show_legend=False)\n",
    "    ax_sh_ts.set_title('(d) Sensible Heat Time Series')\n",
    "    ax_sh_seasonal.set_title('(e) Sensible Heat Seasonal+Diurnal (’02-’07)')\n",
    "    ax_sh_scatter.set_title(f'(f) Sensible Heat Scatter\\nCorrelation (’02-’07): {corr_sh:.3f}')\n",
    "    ax_sh_scatter.set_xlabel('Observed Sensible Heat (W/m²)')\n",
    "    ax_sh_scatter.set_ylabel('Modeled Sensible Heat (W/m²)')\n",
    "    \n",
    "    plot_seasonal(ax_gpp_seasonal, seasonal_obs_gpp, daily_stats_obs_gpp, is_obs=True)\n",
    "    plot_seasonal(ax_gpp_seasonal, seasonal_mod_gpp, daily_stats_mod_gpp, is_obs=False)\n",
    "    ax_gpp_diurnal = ax_gpp_seasonal.twiny()\n",
    "    ax_gpp_diurnal.set_xlim(0, 23)\n",
    "    plot_diurnal(ax_gpp_diurnal, diurnal_obs_gpp, hourly_stats_obs_gpp, diurnal_mod_gpp, hourly_stats_mod_gpp)\n",
    "    \n",
    "    corr_gpp = plot_scatter(ax_gpp_scatter, obs_gpp1, mod_psn1, 'GPP', 'PSN', show_legend=False)\n",
    "    ax_gpp_ts.set_title('(g) GPP Time Series')\n",
    "    ax_gpp_seasonal.set_title('(h) GPP Seasonal+Diurnal (’02-’07)')\n",
    "    ax_gpp_scatter.set_title(f'(i) GPP Scatter\\nCorrelation (’02-’07): {corr_gpp:.3f}')\n",
    "    ax_gpp_scatter.set_xlabel('Observed GPP (μmol/m²/s)')\n",
    "    ax_gpp_scatter.set_ylabel('Modeled PSN (μmol/m²/s)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    plt.show(block=False)  # 非阻塞式显示\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以参考 https://fluxnet.org/data/fluxnet2015-dataset/variables-quick-start-guide\n",
    "\n",
    "保存在ipynb的dpi=60即可，节省空间。最后做出来的海报的dpi = 60。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Constants\n",
    "    SPLIT_INDEX = 24 * 365 * 5  # 5 years of hourly data\n",
    "    figure_dpi = 300\n",
    "    savefig_dpi = 300\n",
    "\n",
    "    FLUX_FILE = \"../obs/US-Syv_2002010106_2009010105_hur_Flux.nc\"\n",
    "    MOD_FILE = \"../US-Syv_01/2002010107.LDASOUT_DOMAIN1\"\n",
    "    \n",
    "    # Process all data\n",
    "    df_obs_lh = process_latent_heat_data(FLUX_FILE)\n",
    "    df_mod_lh, df_mod_sh, df_mod_psn = process_model_data(MOD_FILE)\n",
    "    df_obs_sh = process_sensible_heat_data(FLUX_FILE)\n",
    "    df_obs_gpp = process_gpp_data(FLUX_FILE)\n",
    "    \n",
    "    # Limit observations to model time period\n",
    "    df_obs_lh = df_obs_lh[df_obs_lh.index <= df_mod_lh.index.max()]\n",
    "    df_obs_sh = df_obs_sh[df_obs_sh.index <= df_mod_sh.index.max()]\n",
    "    df_obs_gpp = df_obs_gpp[df_obs_gpp.index <= df_mod_psn.index.max()]\n",
    "    \n",
    "    # Create visualization\n",
    "    create_combined_visualization(\n",
    "        df_obs_lh, df_mod_lh,\n",
    "        df_obs_sh, df_mod_sh,\n",
    "        df_obs_gpp, df_mod_psn,\n",
    "        SPLIT_INDEX, figure_dpi, savefig_dpi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Context' object has no attribute 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8dfaefdc2212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-68aff6458aba>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Process all data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf_obs_lh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_latent_heat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLUX_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdf_mod_lh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_mod_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_mod_psn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_model_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOD_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf_obs_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_sensible_heat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLUX_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fc73f7547e6f>\u001b[0m in \u001b[0;36mprocess_latent_heat_data\u001b[0;34m(obs_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_latent_heat_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Process observation data for latent heat.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values], \n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/plugins.py\u001b[0m in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mstore_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBufferedIOBase\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mAbstractDataStore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m ):\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mengines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_engines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mengines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/xarray/backends/plugins.py\u001b[0m in \u001b[0;36mlist_engines\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mentrypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"xarray.backends\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mentrypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xarray.backends\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuild_engines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentrypoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36mentry_points\u001b[0;34m()\u001b[0m\n\u001b[1;32m    540\u001b[0m         dist.entry_points for dist in distributions())\n\u001b[1;32m    541\u001b[0m     \u001b[0mby_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     return {\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEntryPoint\u001b[0m \u001b[0mobjects\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mall\u001b[0m \u001b[0minstalled\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \"\"\"\n\u001b[0;32m--> 539\u001b[0;31m     eps = itertools.chain.from_iterable(\n\u001b[0m\u001b[1;32m    540\u001b[0m         dist.entry_points for dist in distributions())\n\u001b[1;32m    541\u001b[0m     \u001b[0mby_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/importlib/metadata.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDistributionFinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         return itertools.chain.from_iterable(\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mresolver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_discover_resolvers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfind_distributions\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mof\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \"\"\"\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPathDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Context' object has no attribute 'pattern'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Method 1: If using pip\n",
    "# This will output all installed packages with their exact versions\n",
    "# !pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
