{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin plot\n",
    "\n",
    "Thanks Mbark for reminding me to do Violin plot\n",
    "\n",
    "I then came up the ideas to to Person correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: netCDF4 in /home/jovyan/.local/lib/python3.8/site-packages (1.7.2)\n",
      "Requirement already satisfied: xarray in /home/jovyan/.local/lib/python3.8/site-packages (2023.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (3.1.2)\n",
      "Requirement already satisfied: numpy in /home/jovyan/.local/lib/python3.8/site-packages (from netCDF4) (1.24.4)\n",
      "Requirement already satisfied: cftime in /home/jovyan/.local/lib/python3.8/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from netCDF4) (2020.11.8)\n",
      "Requirement already satisfied: pandas>=1.3 in /home/jovyan/.local/lib/python3.8/site-packages (from xarray) (2.0.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/jovyan/.local/lib/python3.8/site-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.4.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jovyan/.local/lib/python3.8/site-packages (from pandas>=1.3->xarray) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/jovyan/.local/lib/python3.8/site-packages (from pandas>=1.3->xarray) (2025.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.13.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from kiwisolver>=1.0.1->matplotlib) (21.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install netCDF4 xarray matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc03b2d6948c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bc03b2d6948c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m                 verticalalignment='top', fontsize=10)\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolinplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowmedians\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bodies'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_facecolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mviolinplot\u001b[0;34m(self, dataset, positions, vert, widths, showmeans, showextrema, showmedians, points, bw_method)\u001b[0m\n\u001b[1;32m   8052\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8054\u001b[0;31m         \u001b[0mvpstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolin_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_kde_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8055\u001b[0m         return self.violin(vpstats, positions=positions, vert=vert,\n\u001b[1;32m   8056\u001b[0m                            \u001b[0mwidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshowmeans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mviolin_stats\u001b[0;34m(X, method, points)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;31m# Want X to be a list of data sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reshape_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_reshape_2D\u001b[0;34m(X, name)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \"\"\"\n\u001b[1;32m   1425\u001b[0m     \u001b[0;31m# Iterate over columns for ndarrays, over rows otherwise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 聚合函数\n",
    "# -----------------------------\n",
    "def create_seasonal_data(df: pd.DataFrame, column: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    对输入数据按年日（doy）进行平均，返回一个长度为365的Series。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['doy'] = df.index.dayofyear\n",
    "    # 按天取平均：跨年份求均值\n",
    "    seasonal_mean = df.groupby('doy')[column].mean()\n",
    "    return seasonal_mean\n",
    "\n",
    "def create_diurnal_data(df: pd.DataFrame, column: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    对输入数据按局地时（local hour）进行平均，返回一个长度为24的Series。\n",
    "    \"\"\"\n",
    "    local_offset = -6  # 例如：将UTC时间转换为当地时间（90W -> -6小时）\n",
    "    df = df.copy()\n",
    "    # 计算局地时（取余24确保0-23）\n",
    "    df['hour'] = (df.index.hour + local_offset) % 24\n",
    "    diurnal_mean = df.groupby('hour')[column].mean()\n",
    "    return diurnal_mean\n",
    "\n",
    "# -----------------------------\n",
    "# 数据处理函数\n",
    "# -----------------------------\n",
    "def process_obs_data(obs_file: str) -> tuple:\n",
    "    \"\"\"\n",
    "    处理观测数据：潜热（Qle_cor）、显热（Qh_cor）和GPP（GPP_DT）。\n",
    "    \n",
    "    参数：\n",
    "        obs_file (str): 观测数据文件路径\n",
    "    \n",
    "    返回：\n",
    "        tuple: (df_obs_lh, df_obs_sh, df_obs_gpp)\n",
    "            df_obs_lh: 潜热数据的DataFrame\n",
    "            df_obs_sh: 显热数据的DataFrame\n",
    "            df_obs_gpp: GPP数据的DataFrame\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(obs_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "\n",
    "    # 潜热数据\n",
    "    df_obs_lh = ds['Qle_cor'].to_dataframe()\n",
    "    \n",
    "    # 显热数据\n",
    "    df_obs_sh = pd.DataFrame({'Qh': ds['Qh_cor'].values}, index=ds.time.values)\n",
    "    \n",
    "    # GPP数据\n",
    "    df_obs_gpp = pd.DataFrame({'GPP': ds['GPP_DT'].values}, index=ds.time.values)\n",
    "    \n",
    "    return df_obs_lh, df_obs_sh, df_obs_gpp\n",
    "\n",
    "\n",
    "def process_model_data(mod_file: str) -> tuple:\n",
    "    \"\"\"\n",
    "    处理模式数据：\n",
    "    潜热（LH）、显热（HFX）以及GPP代理（PSN）。\n",
    "    假设 'Times' 为字节串，需要转换为datetime。\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(mod_file)\n",
    "    times = [datetime.strptime(t.decode('utf-8'), '%Y-%m-%d_%H:%M:%S')\n",
    "             for t in ds['Times'].values]\n",
    "    df_lh = pd.DataFrame({'LH': ds['LH'][:, 0, 0].values}, index=times)\n",
    "    df_sh = pd.DataFrame({'HFX': ds['HFX'][:, 0, 0].values}, index=times)\n",
    "    df_psn = pd.DataFrame({'PSN': ds['PSN'][:, 0, 0].values}, index=times)\n",
    "    return df_lh, df_sh, df_psn\n",
    "\n",
    "# -----------------------------\n",
    "# 绘图辅助函数\n",
    "# -----------------------------\n",
    "def add_extras(ax, data, positions):\n",
    "    \"\"\"\n",
    "    在小提琴图上叠加箱线图和统计注释。\n",
    "    \"\"\"\n",
    "    bp = ax.boxplot(data, positions=positions, widths=0.15, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='none', color='black'),\n",
    "                    medianprops=dict(color='black'),\n",
    "                    whiskerprops=dict(color='black'),\n",
    "                    capprops=dict(color='black'),\n",
    "                    flierprops=dict(marker='o', markersize=3, markerfacecolor='gray', alpha=0.5))\n",
    "    colors = ['#1f77b4', '#ff7f0e']\n",
    "    for pos, dataset, color in zip(positions, data, colors):\n",
    "        mean_val = np.mean(dataset)\n",
    "        std_val = np.std(dataset)\n",
    "        n_val = len(dataset)\n",
    "        y_pos = np.percentile(dataset, 65)\n",
    "        ax.text(pos-0.13, y_pos, f'Mean: {mean_val:.2f}\\nStd: {std_val:.2f}\\nN: {n_val}', \n",
    "                horizontalalignment='right', fontsize=8, color='black')\n",
    "\n",
    "# -----------------------------\n",
    "# 主函数：3×3网格绘图\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 文件路径（根据需要调整）\n",
    "    FLUX_FILE = \"../obs/US-Syv_2002010106_2009010105_hur_Flux.nc\"\n",
    "    MOD_FILE  = \"../US-Syv_01/2002010107.LDASOUT_DOMAIN1\"\n",
    "    \n",
    "    # 加载观测数据\n",
    "    df_obs_lh, df_obs_sh, df_obs_gpp = process_obs_data(FLUX_FILE)\n",
    "    \n",
    "    # 加载模式数据\n",
    "    df_mod_lh, df_mod_sh, df_mod_psn = process_model_data(MOD_FILE)\n",
    "    \n",
    "    # 过滤掉2007和2008的数据\n",
    "    df_obs_lh  = df_obs_lh[df_obs_lh.index.year < 2007]\n",
    "    df_obs_sh  = df_obs_sh[df_obs_sh.index.year < 2007]\n",
    "    df_obs_gpp = df_obs_gpp[df_obs_gpp.index.year < 2007]\n",
    "    \n",
    "    df_mod_lh  = df_mod_lh[df_mod_lh.index.year < 2007]\n",
    "    df_mod_sh  = df_mod_sh[df_mod_sh.index.year < 2007]\n",
    "    df_mod_psn = df_mod_psn[df_mod_psn.index.year < 2007]\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e']  # 蓝色代表观测，橙色代表模式\n",
    "    \n",
    "    # 定义各变量的信息：名称、对应数据及y轴标签\n",
    "    variables = [\n",
    "        {'name': 'LH', 'obs_df': df_obs_lh, 'obs_col': 'Qle_cor',\n",
    "         'mod_df': df_mod_lh, 'mod_col': 'LH', 'ylabel': 'LH (W/m²)'},\n",
    "        {'name': 'SH', 'obs_df': df_obs_sh, 'obs_col': 'Qh',\n",
    "         'mod_df': df_mod_sh, 'mod_col': 'HFX', 'ylabel': 'SH (W/m²)'},\n",
    "        {'name': 'GPP', 'obs_df': df_obs_gpp, 'obs_col': 'GPP',\n",
    "         'mod_df': df_mod_psn, 'mod_col': 'PSN', 'ylabel': 'GPP (μmol/m²/s)'}\n",
    "    ]\n",
    "    \n",
    "    # 创建3×3子图网格：行=变量，列=数据类型\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    \n",
    "    # col_titles = [\"原始数据（小时分辨率）\", \"DOY均值数据（日分辨率）\", \"局地时均值数据（小时分辨率）\"]\n",
    "    col_titles = [\"Original Data (Hourly)\", \"DOY-Averaged Data (Daily)\", \"Local Hour-Averaged Data (Hourly)\"]\n",
    "    \n",
    "    for j in range(3):\n",
    "        axes[0, j].set_title(col_titles[j])\n",
    "    \n",
    "    # 对每个变量（行）分别绘图\n",
    "    for i, var in enumerate(variables):\n",
    "        # --- 列1：原始数据（小时分辨率） ---\n",
    "        data_orig_obs = var['obs_df'][var['obs_col']].dropna().values\n",
    "        data_orig_mod = var['mod_df'][var['mod_col']].dropna().values\n",
    "        \n",
    "        # --- 列2：DOY均值数据（每日平均，365个点） ---\n",
    "        seasonal_obs = create_seasonal_data(var['obs_df'], var['obs_col'])\n",
    "        seasonal_mod = create_seasonal_data(var['mod_df'], var['mod_col'])\n",
    "        data_seasonal_obs = seasonal_obs.dropna().values  # 应为365个点\n",
    "        data_seasonal_mod = seasonal_mod.dropna().values\n",
    "        \n",
    "        # --- 列3：局地时均值数据（小时平均，24个点） ---\n",
    "        diurnal_obs = create_diurnal_data(var['obs_df'], var['obs_col'])\n",
    "        diurnal_mod = create_diurnal_data(var['mod_df'], var['mod_col'])\n",
    "        data_diurnal_obs = diurnal_obs.dropna().values  # 应为24个点\n",
    "        data_diurnal_mod = diurnal_mod.dropna().values\n",
    "        \n",
    "        # 按列打包数据\n",
    "        datasets = [\n",
    "            [data_orig_obs, data_orig_mod],\n",
    "            [data_seasonal_obs, data_seasonal_mod],\n",
    "            [data_diurnal_obs, data_diurnal_mod]\n",
    "        ]\n",
    "        \n",
    "        # 循环绘制每一列\n",
    "        for j in range(3):\n",
    "            ax = axes[i, j]\n",
    "            data_pair = datasets[j]\n",
    "            \n",
    "            # Generate subplot letter: (a), (b), ... (assuming row-major order in a 3x3 grid)\n",
    "            letter = f\"({chr(97 + i*3 + j)})\"\n",
    "\n",
    "            # Create annotation text that includes the letter, r, and p-value.\n",
    "            annot_text = f\"{letter}\\n\"\n",
    "            ax.text(0.05, 0.95, annot_text, transform=ax.transAxes,\n",
    "                verticalalignment='top', fontsize=10)\n",
    "        \n",
    "            parts = ax.violinplot(data_pair, positions=[1, 2], showmeans=False, showmedians=False)\n",
    "            for k, pc in enumerate(parts['bodies']):\n",
    "                pc.set_facecolor(colors[k])\n",
    "                pc.set_edgecolor('black')\n",
    "                pc.set_alpha(0.7)\n",
    "            add_extras(ax, data_pair, [1, 2])\n",
    "            ax.set_xticks([1, 2])\n",
    "            ax.set_xticklabels(['Obs', 'Mod'])\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(var['ylabel'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_print0(**kwargs):\n",
    "    \"\"\"\n",
    "    Print variable names and their corresponding outputs.\n",
    "\n",
    "    Usage:\n",
    "        multi_print(df_obs_lh=df_obs_lh, df_obs_sh=df_obs_sh, df_obs_gpp=df_obs_gpp)\n",
    "    \"\"\"\n",
    "    for var_name, value in kwargs.items():\n",
    "        print(f\"{var_name}: {value}\")\n",
    "\n",
    "# Example usage:\n",
    "# multi_print(df_obs_lh=df_obs_lh, df_obs_sh=df_obs_sh, df_obs_gpp=df_obs_gpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import re\n",
    "import scipy\n",
    "\n",
    "def multi_print(*args):\n",
    "    \"\"\"\n",
    "    Print variable names and their values.\n",
    "\n",
    "    Note: This function uses the inspect module to extract variable names\n",
    "    from the caller's source code. It works for simple cases but may fail\n",
    "    if the call spans multiple lines or if the arguments are complex expressions.\n",
    "\n",
    "    Usage:\n",
    "        multi_print(df_obs_lh, df_obs_sh, df_obs_gpp)\n",
    "    \"\"\"\n",
    "    # Get the previous frame where multi_print was called\n",
    "    frame = inspect.currentframe().f_back\n",
    "    # Get the line of code that called multi_print\n",
    "    code_context = inspect.getframeinfo(frame).code_context\n",
    "    if code_context:\n",
    "        call_line = code_context[0].strip()\n",
    "        # Use regex to capture the arguments passed to multi_print\n",
    "        match = re.search(r'multi_print\\s*\\((.*)\\)', call_line)\n",
    "        if match:\n",
    "            arg_str = match.group(1)\n",
    "            # Naively split by commas; this assumes no nested commas in the arguments.\n",
    "            var_names = [name.strip() for name in arg_str.split(',')]\n",
    "        else:\n",
    "            var_names = [f'var{i}' for i in range(len(args))]\n",
    "    else:\n",
    "        var_names = [f'var{i}' for i in range(len(args))]\n",
    "\n",
    "    # Print each variable name and its corresponding value\n",
    "    for name, value in zip(var_names, args):\n",
    "        print(f\"{name}: {value}\")\n",
    "\n",
    "# Example usage:\n",
    "# multi_print(df_obs_lh, df_obs_sh, df_obs_gpp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 主函数：3×3网格绘图（相关性图版）\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # 文件路径（根据需要调整）\n",
    "    FLUX_FILE = \"../obs/US-Syv_2002010106_2009010105_hur_Flux.nc\"\n",
    "    MOD_FILE  = \"../US-Syv_01/2002010107.LDASOUT_DOMAIN1\"\n",
    "    \n",
    "    # 加载观测数据\n",
    "    df_obs_lh, df_obs_sh, df_obs_gpp = process_obs_data(FLUX_FILE)\n",
    "    # multi_print(df_obs_lh, df_obs_sh, df_obs_gpp)\n",
    " \n",
    "    # 加载模式数据\n",
    "    df_mod_lh, df_mod_sh, df_mod_psn = process_model_data(MOD_FILE)\n",
    "    # multi_print(df_mod_lh, df_mod_sh, df_mod_psn)\n",
    "    \n",
    "    # 过滤掉2007和2008的数据\n",
    "    df_obs_lh  = df_obs_lh[df_obs_lh.index.year < 2007]\n",
    "    df_obs_sh  = df_obs_sh[df_obs_sh.index.year < 2007]\n",
    "    df_obs_gpp = df_obs_gpp[df_obs_gpp.index.year < 2007]\n",
    "    df_mod_lh  = df_mod_lh[df_mod_lh.index.year < 2007]\n",
    "    df_mod_sh  = df_mod_sh[df_mod_sh.index.year < 2007]\n",
    "    df_mod_psn = df_mod_psn[df_mod_psn.index.year < 2007]\n",
    "    multi_print(df_mod_lh, df_mod_sh, df_mod_psn)\n",
    "\n",
    "    # 求出观测和模式数据的公共时间索引\n",
    "    common_index = df_obs_lh.index.intersection(df_mod_lh.index)\n",
    "\n",
    "    # 根据公共索引重新选取观测数据\n",
    "    df_obs_lh = df_obs_lh.loc[common_index]\n",
    "    df_obs_sh = df_obs_sh.loc[common_index]\n",
    "    df_obs_gpp = df_obs_gpp.loc[common_index]\n",
    "\n",
    "    # 如果需要，也可以对模式数据进行同样的对齐\n",
    "    df_mod_lh = df_mod_lh.loc[common_index]\n",
    "    df_mod_sh = df_mod_sh.loc[common_index]\n",
    "    df_mod_psn = df_mod_psn.loc[common_index]\n",
    "\n",
    "    multi_print(df_obs_lh, df_obs_sh, df_obs_gpp)\n",
    "    \n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e']  # 可用于其他用途\n",
    "    \n",
    "    # 定义各变量的信息：名称、对应数据及轴标签\n",
    "    variables = [\n",
    "        {\n",
    "            'name': 'LH', \n",
    "            'obs_df': df_obs_lh, \n",
    "            'obs_col': 'Qle_cor',\n",
    "            'mod_df': df_mod_lh, \n",
    "            'mod_col': 'LH', \n",
    "            'obs_label': 'Observed Qle_cor (W/m²)',\n",
    "            'mod_label': 'Modeled LH (W/m²)'\n",
    "        },\n",
    "        {\n",
    "            'name': 'SH', \n",
    "            'obs_df': df_obs_sh, \n",
    "            'obs_col': 'Qh',\n",
    "            'mod_df': df_mod_sh, \n",
    "            'mod_col': 'HFX', \n",
    "            'obs_label': 'Observed Qh (W/m²)',\n",
    "            'mod_label': 'Modeled HFX (W/m²)'\n",
    "        },\n",
    "        {\n",
    "            'name': 'GPP', \n",
    "            'obs_df': df_obs_gpp, \n",
    "            'obs_col': 'GPP',\n",
    "            'mod_df': df_mod_psn, \n",
    "            'mod_col': 'PSN', \n",
    "            'obs_label': 'Observed GPP (μmol/m²/s)',\n",
    "            'mod_label': 'Modeled PSN (μmol/m²/s)'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # 创建3×3子图网格：行=变量，列=数据类型\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    \n",
    "    col_titles = [\"Original Data (Hourly)\", \"DOY-Averaged Data (Daily)\", \"Local Hour-Averaged Data (Hourly)\"]\n",
    "    \n",
    "    for j in range(3):\n",
    "        axes[0, j].set_title(col_titles[j])\n",
    "    \n",
    "    # 对每个变量（行）分别绘图\n",
    "    for i, var in enumerate(variables):\n",
    "        # --- 列1：原始数据（小时分辨率） ---\n",
    "        data_orig_obs = var['obs_df'][var['obs_col']].dropna().values\n",
    "        data_orig_mod = var['mod_df'][var['mod_col']].dropna().values\n",
    "        \n",
    "        # --- 列2：DOY均值数据（每日平均，365个点） ---\n",
    "        seasonal_obs = create_seasonal_data(var['obs_df'], var['obs_col'])\n",
    "        seasonal_mod = create_seasonal_data(var['mod_df'], var['mod_col'])\n",
    "        data_seasonal_obs = seasonal_obs.dropna().values  \n",
    "        data_seasonal_mod = seasonal_mod.dropna().values\n",
    "        \n",
    "        # --- 列3：局地时均值数据（小时平均，24个点） ---\n",
    "        diurnal_obs = create_diurnal_data(var['obs_df'], var['obs_col'])\n",
    "        diurnal_mod = create_diurnal_data(var['mod_df'], var['mod_col'])\n",
    "        data_diurnal_obs = diurnal_obs.dropna().values  \n",
    "        data_diurnal_mod = diurnal_mod.dropna().values\n",
    "        \n",
    "        # 按列打包数据\n",
    "        datasets = [\n",
    "            [data_orig_obs, data_orig_mod],\n",
    "            [data_seasonal_obs, data_seasonal_mod],\n",
    "            [data_diurnal_obs, data_diurnal_mod]\n",
    "        ]\n",
    "        \n",
    "        # 循环绘制每一列，改为绘制相关性散点图\n",
    "        for j in range(3):\n",
    "            ax = axes[i, j]\n",
    "            # 清空旧图内容（如果之前有绘制）\n",
    "            ax.cla()\n",
    "            \n",
    "            # 提取观测和模式数据\n",
    "            xdata, ydata = datasets[j][0], datasets[j][1]\n",
    "            \n",
    "            # 绘制散点图\n",
    "            ax.scatter(xdata, ydata, s=10, color='blue', alpha=0.7)\n",
    "            \n",
    "            # # 计算Pearson相关系数\n",
    "            # if len(xdata) > 1 and len(ydata) > 1:\n",
    "            #     r = np.corrcoef(xdata, ydata)[0, 1]\n",
    "            # else:\n",
    "            #     r = np.nan\n",
    "            # ax.text(0.05, 0.95, f'r = {r:.3f}', transform=ax.transAxes,\n",
    "            #         verticalalignment='top', fontsize=10,\n",
    "            #         bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "            # Compute Pearson correlation coefficient and p-value\n",
    "            if len(xdata) > 1 and len(ydata) > 1:\n",
    "                r, p = scipy.stats.pearsonr(xdata, ydata)\n",
    "            else:\n",
    "                r, p = np.nan, np.nan\n",
    "\n",
    "            # Generate subplot letter: (a), (b), ... (assuming row-major order in a 3x3 grid)\n",
    "            letter = f\"({chr(97 + i*3 + j)})\"\n",
    "\n",
    "            # Create annotation text that includes the letter, r, and p-value.\n",
    "            annot_text = f\"{letter}\\nr = {r:.3f}\\np = {p:.3e}\"\n",
    "            ax.text(0.05, 0.95, annot_text, transform=ax.transAxes,\n",
    "                    verticalalignment='top', fontsize=10,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))\n",
    "        \n",
    "            # 添加1:1参考线\n",
    "            combined = np.concatenate([xdata, ydata])\n",
    "            min_val, max_val = np.nanmin(combined), np.nanmax(combined)\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=1)\n",
    "            \n",
    "            # 设置坐标轴标签：下方一行显示x轴标签，首列显示y轴标签\n",
    "            # ax.cla()\n",
    "            if i == 0:\n",
    "                ax.set_title(col_titles[j])\n",
    "            if i == 2:\n",
    "                ax.set_xlabel(var['obs_label'])\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(var['mod_label'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "61358/(365*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "43819/(365*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test spikes\n",
    "\n",
    "Liang: Spikes in Observations: The observed fluxes show more spikes compared to the simulations. Are these real? How is energy closure handled in these data? If these spikes are real, you could use specific statistical metrics to evaluate how well the model captures these peak values. There are methods available to assess this aspect of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code OK （only method 1： 3 sigma method）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Data Processing Functions\n",
    "# -----------------------------\n",
    "def process_latent_heat_data(obs_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for latent heat.\"\"\"\n",
    "    ds = xr.open_dataset(obs_file)\n",
    "    # Convert time if stored as numbers (e.g., 2002010106)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return ds['Qle_cor'].to_dataframe()\n",
    "\n",
    "def process_sensible_heat_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for sensible heat.\"\"\"\n",
    "    ds = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return pd.DataFrame({'Qh': ds['Qh_cor'].values}, index=ds.time.values)\n",
    "\n",
    "def process_gpp_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for GPP.\"\"\"\n",
    "    ds = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return pd.DataFrame({'GPP': ds['GPP_DT'].values}, index=ds.time.values)\n",
    "\n",
    "def process_model_data(mod_file: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Process model data for LH, Sensible Heat (HFX), and GPP proxy (PSN).\n",
    "    The 'Times' variable is assumed to be in bytes and decoded to a string.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(mod_file)\n",
    "    times = [datetime.strptime(t.decode('utf-8'), '%Y-%m-%d_%H:%M:%S')\n",
    "             for t in ds['Times'].values]\n",
    "    df_lh = pd.DataFrame({'LH': ds['LH'][:, 0, 0].values}, index=times)\n",
    "    df_sh = pd.DataFrame({'HFX': ds['HFX'][:, 0, 0].values}, index=times)\n",
    "    df_psn = pd.DataFrame({'PSN': ds['PSN'][:, 0, 0].values}, index=times)\n",
    "    return df_lh, df_sh, df_psn\n",
    "\n",
    "# -----------------------------\n",
    "# Spike Detection and Plotting\n",
    "# -----------------------------\n",
    "def detect_spikes(series: pd.Series):\n",
    "    \"\"\"\n",
    "    Identify spikes defined as any point > (mean + 3 * std).\n",
    "    Returns:\n",
    "      - spike_indices: the index labels of spikes\n",
    "      - spike_values: the spike amplitudes\n",
    "      - mean_val: the mean of the series\n",
    "      - sigma: standard deviation\n",
    "      - threshold: mean + 3 * sigma\n",
    "    \"\"\"\n",
    "    mean_val = series.mean()\n",
    "    sigma = series.std()\n",
    "    threshold = mean_val + 3 * sigma\n",
    "    spike_mask = series > threshold\n",
    "    spike_indices = series.index[spike_mask]\n",
    "    spike_values = series[spike_mask]\n",
    "    return spike_indices, spike_values, mean_val, sigma, threshold\n",
    "\n",
    "def plot_variable_spikes(ax, obs_series: pd.Series, mod_series: pd.Series, var_name: str, show_labels: bool = False):\n",
    "    \"\"\"\n",
    "    Plot the time series for a given variable along with spike analysis.\n",
    "    - Plots the observed series (blue) and modeled series (orange).\n",
    "    - Draws horizontal lines at mean+1σ (dotted), mean+2σ (dash-dot), and mean+3σ (dashed)\n",
    "      separately for obs (red) and mod (darkred).\n",
    "    - Overlays spike markers (points above the 3σ threshold) in red colors.\n",
    "    - Prints spike statistics to the console.\n",
    "    \"\"\"\n",
    "    # --- Observed Data ---\n",
    "    obs_spike_idx, obs_spike_vals, obs_mean, obs_sigma, obs_thresh = detect_spikes(obs_series)\n",
    "    obs_T1 = obs_mean + 1 * obs_sigma\n",
    "    obs_T2 = obs_mean + 2 * obs_sigma\n",
    "    # obs_T3 is the spike threshold = obs_mean + 3σ\n",
    "\n",
    "    # --- Modeled Data ---\n",
    "    mod_spike_idx, mod_spike_vals, mod_mean, mod_sigma, mod_thresh = detect_spikes(mod_series)\n",
    "    mod_T1 = mod_mean + 1 * mod_sigma\n",
    "    mod_T2 = mod_mean + 2 * mod_sigma\n",
    "    # mod_T3 is the spike threshold = mod_mean + 3σ\n",
    "\n",
    "    # Plot the time series lines\n",
    "    ax.plot(obs_series.index, obs_series.values, color='#1f77b4', label=f'Obs {var_name}')\n",
    "    ax.plot(mod_series.index, mod_series.values, color='#ff7f0e', label=f'Mod {var_name}')\n",
    "\n",
    "    # Plot threshold lines for observed (red shades)\n",
    "    ax.axhline(obs_T1, color='red', linestyle=':', linewidth=1,\n",
    "               label='Obs 1σ' if show_labels else None)\n",
    "    ax.axhline(obs_T2, color='red', linestyle='-.', linewidth=1,\n",
    "               label='Obs 2σ' if show_labels else None)\n",
    "    ax.axhline(obs_thresh, color='red', linestyle='--', linewidth=1,\n",
    "               label='Obs 3σ (spike threshold)' if show_labels else None)\n",
    "\n",
    "    # Plot threshold lines for modeled (darkred shades)\n",
    "    ax.axhline(mod_T1, color='darkred', linestyle=':', linewidth=1,\n",
    "               label='Mod 1σ' if show_labels else None)\n",
    "    ax.axhline(mod_T2, color='darkred', linestyle='-.', linewidth=1,\n",
    "               label='Mod 2σ' if show_labels else None)\n",
    "    ax.axhline(mod_thresh, color='darkred', linestyle='--', linewidth=1,\n",
    "               label='Mod 3σ (spike threshold)' if show_labels else None)\n",
    "\n",
    "    # Overlay spike markers (using distinct red colors)\n",
    "    ax.scatter(obs_spike_idx, obs_spike_vals, color='red', marker='o', s=30,\n",
    "               label='Obs Spikes' if show_labels else None)\n",
    "    ax.scatter(mod_spike_idx, mod_spike_vals, color='darkred', marker='o', s=30,\n",
    "               label='Mod Spikes' if show_labels else None)\n",
    "\n",
    "    # Axis formatting\n",
    "    ax.set_title(f'{var_name} Spike Analysis')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'{var_name} Flux')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Print spike analysis results to console\n",
    "    num_obs_spikes = len(obs_spike_idx)\n",
    "    mean_obs_spike = obs_spike_vals.mean() if num_obs_spikes > 0 else np.nan\n",
    "\n",
    "    num_mod_spikes = len(mod_spike_idx)\n",
    "    mean_mod_spike = mod_spike_vals.mean() if num_mod_spikes > 0 else np.nan\n",
    "\n",
    "    print(f\"Spike Analysis for {var_name}:\")\n",
    "    print(f\"  Observed: spikes = {num_obs_spikes}, Mean amplitude = {mean_obs_spike:.2f}, \"\n",
    "          f\"σ = {obs_sigma:.2f}, 1σ = {obs_T1:.2f}, 2σ = {obs_T2:.2f}, 3σ = {obs_thresh:.2f}\")\n",
    "    print(f\"  Modeled:  spikes = {num_mod_spikes}, Mean amplitude = {mean_mod_spike:.2f}, \"\n",
    "          f\"σ = {mod_sigma:.2f}, 1σ = {mod_T1:.2f}, 2σ = {mod_T2:.2f}, 3σ = {mod_thresh:.2f}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main Routine: Build Simple Figure\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # File paths (adjust as needed)\n",
    "    FLUX_FILE = \"../obs/US-Syv_2002010106_2009010105_hur_Flux.nc\"\n",
    "    MOD_FILE  = \"../US-Syv_01/2002010107.LDASOUT_DOMAIN1\"\n",
    "    \n",
    "    # Load observation data\n",
    "    df_obs_lh = process_latent_heat_data(FLUX_FILE)\n",
    "    df_obs_sh = process_sensible_heat_data(FLUX_FILE)\n",
    "    df_obs_gpp = process_gpp_data(FLUX_FILE)\n",
    "    \n",
    "    # Load model data\n",
    "    df_mod_lh, df_mod_sh, df_mod_psn = process_model_data(MOD_FILE)\n",
    "    \n",
    "    # (Optional) Limit observations to model period if needed\n",
    "    max_time = min(df_mod_lh.index.max(), df_mod_sh.index.max(), df_mod_psn.index.max())\n",
    "    df_obs_lh = df_obs_lh[df_obs_lh.index <= max_time]\n",
    "    df_obs_sh = df_obs_sh[df_obs_sh.index <= max_time]\n",
    "    df_obs_gpp = df_obs_gpp[df_obs_gpp.index <= max_time]\n",
    "    \n",
    "    # Create a figure with 3 subplots (one column)\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "    \n",
    "    # For the first subplot (LH), include full legend labels\n",
    "    plot_variable_spikes(axes[0],\n",
    "                           obs_series=df_obs_lh['Qle_cor'],\n",
    "                           mod_series=df_mod_lh['LH'],\n",
    "                           var_name=\"LH\",\n",
    "                           show_labels=True)\n",
    "    \n",
    "    # Second subplot: Sensible Heat (SH)\n",
    "    plot_variable_spikes(axes[1],\n",
    "                           obs_series=df_obs_sh['Qh'],\n",
    "                           mod_series=df_mod_sh['HFX'],\n",
    "                           var_name=\"SH\",\n",
    "                           show_labels=False)\n",
    "    \n",
    "    # Third subplot: GPP (use observation from GPP and model from PSN)\n",
    "    plot_variable_spikes(axes[2],\n",
    "                           obs_series=df_obs_gpp['GPP'],\n",
    "                           mod_series=df_mod_psn['PSN'],\n",
    "                           var_name=\"GPP\",\n",
    "                           show_labels=False)\n",
    "    \n",
    "    # Place a common legend (only once)\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=4, frameon=True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test more times\n",
    "下面给出一个新的 detect_spikes() 函数版本，该函数新增一个整型参数 method，取值 1～6 分别对应下面列出的 6 种检测方法：\n",
    "\n",
    "method=1：基于全局均值和标准差的 “mean+3σ” 方法\n",
    "method=2：Z-score 方法（默认阈值为 3）\n",
    "method=3：基于中位数和 MAD 的方法（默认阈值 3.5）\n",
    "method=4：基于局部（滚动）均值和标准差的方法（默认窗口 24，阈值 3）\n",
    "method=5：利用 scikit-learn 的 IsolationForest 方法（默认污染率 0.01）\n",
    "method=6：利用 SciPy 的 find_peaks() 方法（默认 prominence=1.0）\n",
    "下面是新的 detect_spikes() 函数代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Data Processing Functions\n",
    "# -----------------------------\n",
    "def process_latent_heat_data(obs_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for latent heat.\"\"\"\n",
    "    ds = xr.open_dataset(obs_file)\n",
    "    # Convert time if stored as numbers (e.g., 2002010106)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return ds['Qle_cor'].to_dataframe()\n",
    "\n",
    "def process_sensible_heat_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for sensible heat.\"\"\"\n",
    "    ds = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return pd.DataFrame({'Qh': ds['Qh_cor'].values}, index=ds.time.values)\n",
    "\n",
    "def process_gpp_data(flux_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Process observation data for GPP.\"\"\"\n",
    "    ds = xr.open_dataset(flux_file)\n",
    "    if isinstance(ds.time.values[0], np.floating):\n",
    "        times = pd.to_datetime([f\"{int(t):010d}\" for t in ds.time.values],\n",
    "                               format='%Y%m%d%H')\n",
    "        ds['time'] = times\n",
    "    return pd.DataFrame({'GPP': ds['GPP_DT'].values}, index=ds.time.values)\n",
    "\n",
    "def process_model_data(mod_file: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Process model data for LH, Sensible Heat (HFX), and GPP proxy (PSN).\n",
    "    The 'Times' variable is assumed to be in bytes and decoded to a string.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(mod_file)\n",
    "    times = [datetime.strptime(t.decode('utf-8'), '%Y-%m-%d_%H:%M:%S')\n",
    "             for t in ds['Times'].values]\n",
    "    df_lh = pd.DataFrame({'LH': ds['LH'][:, 0, 0].values}, index=times)\n",
    "    df_sh = pd.DataFrame({'HFX': ds['HFX'][:, 0, 0].values}, index=times)\n",
    "    df_psn = pd.DataFrame({'PSN': ds['PSN'][:, 0, 0].values}, index=times)\n",
    "    return df_lh, df_sh, df_psn\n",
    "\n",
    "def detect_spikes(series: pd.Series, method: int = 1, **kwargs):\n",
    "    \"\"\"\n",
    "    Detect spikes in a time series using different methods.\n",
    "    \n",
    "    Parameters:\n",
    "        series : pd.Series\n",
    "            要检测的时间序列数据。\n",
    "        method : int, default=1\n",
    "            选择检测方法：\n",
    "              1 - 全局均值+3σ 方法\n",
    "              2 - Z-score 方法（默认阈值 3）\n",
    "              3 - MAD 方法（默认阈值 3.5）\n",
    "              4 - 滚动统计方法（默认窗口 24，阈值 3）\n",
    "              5 - IsolationForest 方法（默认 contamination=0.01）\n",
    "              6 - scipy.signal.find_peaks 方法（默认 prominence=1.0）\n",
    "        **kwargs : dict\n",
    "            各方法的额外参数，例如 threshold、window、contamination、prominence 等。\n",
    "    \n",
    "    Returns:\n",
    "        spike_indices : Index\n",
    "            被检测为“spike”的时间索引。\n",
    "        spike_values : pd.Series\n",
    "            对应的数值。\n",
    "        extra_info : dict\n",
    "            其他附加信息，例如计算得到的均值、σ、阈值等。\n",
    "    \"\"\"\n",
    "    if method == 1:\n",
    "        # 方法1：均值+3σ\n",
    "        mean_val = series.mean()\n",
    "        sigma = series.std()\n",
    "        threshold = mean_val + 3 * sigma\n",
    "        spike_mask = series > threshold\n",
    "        spike_indices = series.index[spike_mask]\n",
    "        spike_values = series[spike_mask]\n",
    "        extra_info = {'mean': mean_val, 'sigma': sigma, 'threshold': threshold, 'method': 'mean+3σ'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    elif method == 2:\n",
    "        # 方法2：Z-score 方法，默认阈值 3\n",
    "        threshold = kwargs.get('threshold', 3.0)\n",
    "        mean_val = series.mean()\n",
    "        sigma = series.std()\n",
    "        z_scores = (series - mean_val) / sigma\n",
    "        spike_mask = abs(z_scores) > threshold\n",
    "        spike_indices = series.index[spike_mask]\n",
    "        spike_values = series[spike_mask]\n",
    "        extra_info = {'mean': mean_val, 'sigma': sigma, 'z_threshold': threshold, 'method': 'zscore'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    elif method == 3:\n",
    "        # 方法3：MAD 方法，默认阈值 3.5\n",
    "        threshold = kwargs.get('threshold', 3.5)\n",
    "        median = series.median()\n",
    "        mad = (np.abs(series - median)).median()\n",
    "        modified_z_score = 0.6745 * (series - median) / mad\n",
    "        spike_mask = np.abs(modified_z_score) > threshold\n",
    "        spike_indices = series.index[spike_mask]\n",
    "        spike_values = series[spike_mask]\n",
    "        extra_info = {'median': median, 'mad': mad, 'mad_threshold': threshold, 'method': 'MAD'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    elif method == 4:\n",
    "        # 方法4：滚动统计方法（局部均值+局部标准差），默认窗口 24，阈值 3\n",
    "        window = kwargs.get('window', 24)\n",
    "        threshold = kwargs.get('threshold', 3.0)\n",
    "        roll_mean = series.rolling(window=window, center=True).mean()\n",
    "        roll_std = series.rolling(window=window, center=True).std()\n",
    "        spike_mask = series > (roll_mean + threshold * roll_std)\n",
    "        spike_indices = series.index[spike_mask]\n",
    "        spike_values = series[spike_mask]\n",
    "        extra_info = {'window': window, 'threshold': threshold, 'method': 'rolling'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    elif method == 5:\n",
    "        # 方法5：IsolationForest 方法\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        contamination = kwargs.get('contamination', 0.01)\n",
    "        X = series.values.reshape(-1, 1)\n",
    "        clf = IsolationForest(contamination=contamination, random_state=42)\n",
    "        clf.fit(X)\n",
    "        preds = clf.predict(X)\n",
    "        spike_mask = preds == -1\n",
    "        spike_indices = series.index[spike_mask]\n",
    "        spike_values = series[spike_mask]\n",
    "        extra_info = {'contamination': contamination, 'method': 'IsolationForest'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    elif method == 6:\n",
    "        # 方法6：scipy.signal.find_peaks 方法，默认 prominence=1.0\n",
    "        from scipy.signal import find_peaks\n",
    "        prominence = kwargs.get('prominence', 1.0)\n",
    "        peaks, properties = find_peaks(series.values, prominence=prominence)\n",
    "        spike_indices = series.index[peaks]\n",
    "        spike_values = series.iloc[peaks]\n",
    "        extra_info = {'prominence': prominence, 'properties': properties, 'method': 'find_peaks'}\n",
    "        return spike_indices, spike_values, extra_info\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose an integer between 1 and 6.\")\n",
    "\n",
    "\n",
    "def plot_variable_spikes(ax, obs_series: pd.Series, mod_series: pd.Series, var_name: str,\n",
    "                         spike_method: int = 1, spike_kwargs: dict = None, show_labels: bool = False):\n",
    "    \"\"\"\n",
    "    绘制单个变量的时间序列及其 spike 检测结果。\n",
    "    \n",
    "    参数:\n",
    "      - spike_method: 选择的 spike 检测方法（1～6）。\n",
    "      - spike_kwargs: 传递给 detect_spikes() 的额外参数字典。\n",
    "    \"\"\"\n",
    "    if spike_kwargs is None:\n",
    "        spike_kwargs = {}\n",
    "\n",
    "    # 观测数据的 spike 检测\n",
    "    obs_spike_idx, obs_spike_vals, obs_extra = detect_spikes(obs_series, method=spike_method, **spike_kwargs)\n",
    "    # 模型数据的 spike 检测\n",
    "    mod_spike_idx, mod_spike_vals, mod_extra = detect_spikes(mod_series, method=spike_method, **spike_kwargs)\n",
    "    \n",
    "    # 绘制时间序列曲线（观测：蓝色，模型：橙色）\n",
    "    ax.plot(obs_series.index, obs_series.values, color='#1f77b4', label=f'Obs {var_name}')\n",
    "    ax.plot(mod_series.index, mod_series.values, color='#ff7f0e', label=f'Mod {var_name}')\n",
    "    \n",
    "    # 在此处为了显示阈值水平（如果方法支持全局统计，可以额外绘制），这里只作为示例，\n",
    "    # 若方法返回 extra_info 包含 'threshold' 字段则绘制该水平线（用虚线）。\n",
    "    if 'threshold' in obs_extra:\n",
    "        ax.axhline(obs_extra['threshold'], color='red', linestyle='--', linewidth=1,\n",
    "                   label='Obs Spike Threshold' if show_labels else None)\n",
    "    if 'threshold' in mod_extra:\n",
    "        ax.axhline(mod_extra['threshold'], color='darkred', linestyle='--', linewidth=1,\n",
    "                   label='Mod Spike Threshold' if show_labels else None)\n",
    "    \n",
    "    # 绘制检测到的 spike 点（使用不同的红色）\n",
    "    ax.scatter(obs_spike_idx, obs_spike_vals, color='red', marker='o', s=30,\n",
    "               label='Obs Spikes' if show_labels else None)\n",
    "    ax.scatter(mod_spike_idx, mod_spike_vals, color='darkred', marker='o', s=30,\n",
    "               label='Mod Spikes' if show_labels else None)\n",
    "    \n",
    "    ax.set_title(f'{var_name} Spike Analysis (Method {spike_method})')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'{var_name} Flux')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 打印检测结果（可选）\n",
    "    num_obs = len(obs_spike_idx)\n",
    "    num_mod = len(mod_spike_idx)\n",
    "    mean_obs = obs_spike_vals.mean() if num_obs > 0 else np.nan\n",
    "    mean_mod = mod_spike_vals.mean() if num_mod > 0 else np.nan\n",
    "    print(f\"Spike Analysis for {var_name} using method {spike_method}:\")\n",
    "    print(f\"  Observed: spikes = {num_obs}, Mean amplitude = {mean_obs:.2f}\")\n",
    "    print(f\"  Modeled:  spikes = {num_mod}, Mean amplitude = {mean_mod:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 文件路径（请根据实际情况调整）\n",
    "    FLUX_FILE = \"../obs/US-Syv_2002010106_2009010105_hur_Flux.nc\"\n",
    "    MOD_FILE  = \"../US-Syv_01/2002010107.LDASOUT_DOMAIN1\"\n",
    "    \n",
    "    # 载入观测数据\n",
    "    df_obs_lh = process_latent_heat_data(FLUX_FILE)\n",
    "    df_obs_sh = process_sensible_heat_data(FLUX_FILE)\n",
    "    df_obs_gpp = process_gpp_data(FLUX_FILE)\n",
    "    \n",
    "    # 载入模型数据\n",
    "    df_mod_lh, df_mod_sh, df_mod_psn = process_model_data(MOD_FILE)\n",
    "    \n",
    "    # 限制观测数据的时间范围与模型数据一致\n",
    "    max_time = min(df_mod_lh.index.max(), df_mod_sh.index.max(), df_mod_psn.index.max())\n",
    "    df_obs_lh = df_obs_lh[df_obs_lh.index <= max_time]\n",
    "    df_obs_sh = df_obs_sh[df_obs_sh.index <= max_time]\n",
    "    df_obs_gpp = df_obs_gpp[df_obs_gpp.index <= max_time]\n",
    "    \n",
    "    # 循环使用方法 1 到 6 绘制图像，每种方法生成一个包含 3 个子图的图\n",
    "    for spike_method in range(1, 7):\n",
    "        # 如有需要，可在 spike_kwargs 中设置额外参数，例如：\n",
    "        # spike_kwargs = {'threshold': 3.0} 或 {'prominence': 1.0} 等\n",
    "        spike_kwargs = {}  \n",
    "        \n",
    "        # 创建一个包含 3 个子图（纵向排列）的图像\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "        \n",
    "        # 子图 1: 潜热 (LH)\n",
    "        plot_variable_spikes(axes[0],\n",
    "                             obs_series=df_obs_lh['Qle_cor'],\n",
    "                             mod_series=df_mod_lh['LH'],\n",
    "                             var_name=\"LH\",\n",
    "                             spike_method=spike_method,\n",
    "                             spike_kwargs=spike_kwargs,\n",
    "                             show_labels=True)\n",
    "        \n",
    "        # 子图 2: 显热 (SH)\n",
    "        plot_variable_spikes(axes[1],\n",
    "                             obs_series=df_obs_sh['Qh'],\n",
    "                             mod_series=df_mod_sh['HFX'],\n",
    "                             var_name=\"SH\",\n",
    "                             spike_method=spike_method,\n",
    "                             spike_kwargs=spike_kwargs,\n",
    "                             show_labels=False)\n",
    "        \n",
    "        # 子图 3: GPP（观测用 GPP，模型用 PSN）\n",
    "        plot_variable_spikes(axes[2],\n",
    "                             obs_series=df_obs_gpp['GPP'],\n",
    "                             mod_series=df_mod_psn['PSN'],\n",
    "                             var_name=\"GPP\",\n",
    "                             spike_method=spike_method,\n",
    "                             spike_kwargs=spike_kwargs,\n",
    "                             show_labels=False)\n",
    "        \n",
    "        # 设置整个图的标题，并调整布局\n",
    "        fig.suptitle(f\"Spike Detection Method {spike_method}\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        \n",
    "        # 显示图像（在 Jupyter Notebook 中每次 plt.show() 都会输出一个图）\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
